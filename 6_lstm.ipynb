{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292420 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.91\n",
      "================================================================================\n",
      "dba  ejskfzlihe vxbke e ei otwoylhmadupgsztg zxonobs aja ytsdl iaqedfrigrzp uzti\n",
      "ij apyyuvbpp czrs ankbogn g caxpha   nyvwpscjblesxqaephepp uoeimznchkioati tofri\n",
      "czipzctmmes qi svnvr zbheel uvyisotkz   ecfhdusbbzannasergwmnkl gmcttjt  t  lbkv\n",
      "bmmwkbv h qcgqmxriqoia bqco cqzeap sirdiigajuewoooradbwml tsfb c n j elap k oe  \n",
      "sn wgeosw qyhfybpze qwas  l do atrmbhd aubbhnr ocp c diwudnbnekebopvr rghi nh go\n",
      "================================================================================\n",
      "Validation set perplexity: 20.05\n",
      "Average loss at step 100: 2.623843 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.16\n",
      "Validation set perplexity: 10.56\n",
      "Average loss at step 200: 2.262993 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.72\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 300: 2.108922 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 400: 2.006626 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 500: 1.940340 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 600: 1.912562 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.859836 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 800: 1.822641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 900: 1.829570 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1000: 1.825152 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "domcy one nine seven four nine seven lide stense one five ficm segret yaly norik\n",
      "gniely nareht of inclirg leations of whenok leang luc texen the seven ely culunt\n",
      "ca of agpes sove four zero two zero one eight nine four to the songe the fux eld\n",
      "e the metian fromen moss ffection of emp sixwedent husifn and behen be ous statm\n",
      "j rivable dess warl pederam trafver dising of deaspate is wine izes and  reafe i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1100: 1.775169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1200: 1.750746 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1300: 1.735485 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1400: 1.748170 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1500: 1.733849 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1600: 1.741563 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1700: 1.710966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 1800: 1.671041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 1900: 1.643714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2000: 1.696359 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "vust carlance morts ongly anny townot in norlatic shaplugn to mowniis king no fr\n",
      " storm allone wending alfo scholl and libuld his vardin crystral the bat in mult\n",
      "ings and dignution or aldies zerronench of stillary on this to followard kles ma\n",
      "s sarding nevituding the as j lining de in the english be eastain gavol sulinite\n",
      "zegen matour tharn on to extoraictt engizing oth subgebreon on theingoy gause xe\n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2100: 1.689494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2200: 1.681228 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2300: 1.638631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2400: 1.663874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2500: 1.678445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2600: 1.653404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 2700: 1.657914 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 2800: 1.649324 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 2900: 1.651628 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3000: 1.653810 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "gue of thesely saccasion s assaan tebrausing polation peste sachlate new purnami\n",
      "becthay accoruld are through any a aljany in there handious a doldw ych his botw\n",
      "er be other dreations becigy new formies of community the remowneds are five is \n",
      "veres of led lating witht pardicely cold passestall a menensts semitil a handing\n",
      "and foundre of fippolitey dopance of two plamim in the scien inclodish south sys\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3100: 1.630238 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3200: 1.643603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3300: 1.639954 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3400: 1.671936 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3500: 1.659553 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3600: 1.666531 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3700: 1.643733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3800: 1.642520 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3900: 1.632545 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4000: 1.649408 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "joven of mocause tripe ide reany manochew situsting caush of the menbently of di\n",
      " serk ot is the official poluted to baykwim markbatell dobotiria hampeer can who\n",
      "niun car s feccex bollres distank trivises time s typical b one nine one eight o\n",
      "ching the otherizates imported got is jospuns he it cuz ganjand for uld to finst\n",
      "que lighterne doew by the historia trde the deumuters of sedr reson peict betopo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4100: 1.630758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.634497 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4300: 1.619411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4400: 1.605359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4500: 1.614000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4600: 1.612327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.620573 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4800: 1.626230 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4900: 1.632142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5000: 1.605986 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "quant in their the hincie the orderned a from y the being of who sell the fairin\n",
      "hia halitions of the not in these unigne these davos biritions cramms presser as\n",
      "pinity b kause concent mistuing the province gomhord achists sest at the seven n\n",
      "le by one three see woels game arver progrease far one six ott ricries in owe in\n",
      "es inflah anced by first worth araw aide ministromer of engrant american that to\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5100: 1.603975 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5200: 1.589399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5300: 1.575810 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5400: 1.574436 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5500: 1.559389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5600: 1.576469 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5700: 1.566804 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5800: 1.579042 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900: 1.571084 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6000: 1.544022 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "ally two basion then bar devest one nine five seven eight two nine nine one zero\n",
      "y was jus regbut phetion can the g one eight seven gnortarieutwaloy devised by f\n",
      "rately yope lays foothur of the encode pig ulactrued or leases being three pers \n",
      "warm done a musking stark to partikacoer the betame the black equirational black\n",
      "s ally alway a prounted in of wide six our seven eight eight the hathing enceile\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.562940 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6200: 1.533740 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6300: 1.541171 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6400: 1.537370 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6500: 1.554796 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6600: 1.593671 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6700: 1.579229 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6800: 1.603041 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6900: 1.580635 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.579192 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "ivated play of have one nine one nine nine min provention ensistey in also allea\n",
      "milary to aprille from plany the modericancs swaree this patir niod of the speak\n",
      "iler this high patliky leaders limks dut noth caprist line the one of takentilus\n",
      "renian prinst inverperpplacerus tudal indian and h was prizces team drian withsl\n",
      "guer over its from the orthement in one two one seven six eight six kifors twan \n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    \n",
    "  ### Concatenating the 4 matrix multiplications\n",
    "  total_x = tf.concat(1, [ix, fx, cx, ox])\n",
    "  total_m = tf.concat(1, [im, fm, cm, om])\n",
    "  total_b = tf.concat(1, [ib, fb, cb, ob])\n",
    "    \n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    total_matmul = tf.matmul(i, total_x) + tf.matmul(o, total_m) + total_b\n",
    "    sig_input, sig_forget, sig_update, sig_output = tf.split(1, 4, total_matmul)\n",
    "    input_gate = tf.sigmoid(sig_input)\n",
    "    forget_gate = tf.sigmoid(sig_forget)\n",
    "    update = sig_update\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(sig_output)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299124 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "funwnpaeoivmxe tjbnsros  iiayiteohn t r ns jixngoifhsgvftx ooyldna r oc outwdfdi\n",
      "zu c xnrgernfpktemgotd cpvejgcsfb mbuyiignoaapepkxqzhczpqxblanfdrkoybdbaeytfr al\n",
      "trdamfgsdvtz trbgvo vqohw  omoxnmesjnxhncbdyitdxhjpmefjmpgfk omx nozgso efwopjnc\n",
      "kpxgmqegciqmvolx niu thykrybtli o gqanqgm clreesqgouwyfozfrkieunmgqyoitlyysi i f\n",
      "e oa p gpbwaog ds eiapwuutoyzigst tmefsaddtsmrvoynen tiesai  hmng bicol xtehejh \n",
      "================================================================================\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 100: 2.589080 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.56\n",
      "Validation set perplexity: 10.55\n",
      "Average loss at step 200: 2.248536 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.28\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 300: 2.086504 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 400: 2.031578 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 500: 1.981568 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600: 1.897758 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 700: 1.872025 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 800: 1.868760 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 900: 1.845698 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1000: 1.847520 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "================================================================================\n",
      "x bork duims and buid the saturate wasts uni ubrade c and be one nine fves one s\n",
      "xes withs in okiging stocu partause ut and rigrtical lacrer in puisiter b one si\n",
      "y pre one than marke syill use of hoskap the moway unorvovidh perpily the soundt\n",
      "teme by ashack and with fimpising nomer of heose in prodener aqwery ruldantatius\n",
      "mental from a werblaty then fhouck imsse and nate inproble develoled udwing juch\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1100: 1.801293 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1200: 1.770930 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1300: 1.757082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1400: 1.762307 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.746922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1600: 1.727685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1700: 1.712392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.687303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.695698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2000: 1.684766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "jacted pohe two zero zero syughn these defect the reall standedages therentanaly\n",
      "bow on interremod coppeent to it all well model a rvitwent by quading be comple \n",
      "ee is and even nork exeruto prytabalal inten d magres as two betsent at feal is \n",
      "quate is tofacthraned untles as close if more cometive in this elactualing these\n",
      "jetory in koclut is the five inf estintpsien stovan m  reteforations setmarsed o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2100: 1.689552 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2200: 1.706359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2300: 1.705371 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2400: 1.690106 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2500: 1.692040 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.671527 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2700: 1.682185 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2800: 1.682847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2900: 1.678670 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3000: 1.681196 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "wate three darba cenition bone one two zero zero two zero vama zero zero southin\n",
      "or seles as one aid to heit of a callext per cpieselver but hymetom bane weles b\n",
      "s including the elf arger this mirrow blook of histed forfe popule a not use eff\n",
      "fer mower the tiles of regreen other action contridues fers us can a pearly over\n",
      "es to jesen those anadive to the american spestory depanne as the usbon abograge\n",
      "================================================================================\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3100: 1.651563 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3200: 1.639439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3300: 1.650025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3400: 1.637237 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3500: 1.676341 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3600: 1.652019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3700: 1.655863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.657281 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3900: 1.649641 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4000: 1.638831 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "fititalo anquarity online acced or pobre to the language use four four rurist sh\n",
      "utists of a tustifisity of the a ne one one seven low watmetime which cleep at t\n",
      "cess dispainiateiste from text aporty that furmowed been site rains such tonau t\n",
      "vice eight four is the one nine nine five sin iin amerfc and the gries the opmen\n",
      "quatle was the histor from a feltus the lemanton gard formeration of the deguled\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4100: 1.617115 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4200: 1.615716 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4300: 1.618788 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4400: 1.604620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4500: 1.641757 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4600: 1.625409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4700: 1.622405 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4800: 1.608911 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4900: 1.619478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5000: 1.615979 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "y and the marferis of a metrine which the spucoort in or yees to required in at \n",
      "thromets allawmon more later three zero day varis the recrace texds and states c\n",
      "jusceshnoobly the machamvorred in the down vewbor smorman was included fich long\n",
      "queded the lexich difflatiol one eight nine nine five five vill area populan rac\n",
      "velleys both inecuing american the form i f transported as was in also cuprance \n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.592197 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5200: 1.593105 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5300: 1.595863 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5400: 1.593256 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5500: 1.591503 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5600: 1.567022 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5700: 1.584525 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5800: 1.603564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5900: 1.583513 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.589807 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "fi of des beganes beounds that the fooperes lazwers his ithinger the pollem it a\n",
      "whan commired in one nine seven four seevan was malaron some silliouson in absta\n",
      "viewy and lock aady this reliament of gravqt and kelled it are aptuds destroem i\n",
      "d the one nine seven zero six the dimat of in eight interritary in this it playe\n",
      "poin who glouggts and sprincce  and general cwotes of not as this arecies to be \n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.579207 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6200: 1.592540 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6300: 1.592591 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6400: 1.573178 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6500: 1.559632 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6600: 1.605214 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6700: 1.574405 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6800: 1.576605 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900: 1.571767 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7000: 1.588947 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "vieing two zhro throigh and syst opeanist klean earth one nine six five governe \n",
      "ca do to llooch the use to the has the noter those absomate courver ze e was loi\n",
      "bclogne mogan wincer tokoum syors generics to inves concegarists extroply itself\n",
      "man yeed in not from noobguaghtl of this mactial ebguors sagier prince bj sectio\n",
      "que one nine nine seven zero nine four foordan commutus officity of depice only \n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented a modified version of Arn-O's model: https://github.com/Arn-O/udacity-deep-learning/blob/master/6_lstm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 0.8\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  ### Change input size to embedding size\n",
    "\n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 15000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.301940 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.17\n",
      "================================================================================\n",
      "gbc  eurxtboteyeswebmjtaevefstejlrsvczkowv ohl ilci ggetnfwue rm kwvedvduheboysyh\n",
      "da ogheeeeik rscfpzvkegudclidzflutdlpnje xrb yifsheamneadpxixoxfbs rmbislfmawetei\n",
      "zmqle c ttwhixt  ekemmhrxdol otr wqo oszxber fzqtdaxhesosotjnpcnaaauc lnukivh  a \n",
      "vqm vnga dt eslpdteghbxivnizpa oe jnmosgiq  hftndgao vf  w  zohwda  m aifkfvieext\n",
      "m ofbm  dhpftnfynzhdkegmtmiatgdabeoneersizliwcdmkefltipvbeaammney eeuisteovfjyiv \n",
      "================================================================================\n",
      "Validation set perplexity: 19.85\n",
      "Average loss at step 100: 2.372400 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.73\n",
      "Validation set perplexity: 9.46\n",
      "Average loss at step 200: 2.094048 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 300: 2.016238 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 400: 1.961480 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 500: 1.926689 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 600: 1.896434 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 700: 1.868734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 800: 1.860615 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 900: 1.834221 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 1000: 1.814128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "dous frome diae systert gurandesacrowed screash gerries mather rebcludecanight ae\n",
      "by the only arysis with thear millian jabonorded fanp malet with can and kgaring \n",
      "ne zero sure idents advoicaur early of notivunime hogenes for adcial one one six \n",
      "dhying uniqaehis the not interetaxnly limt nonory herposelly is the churop nolots\n",
      "uce bluenpnwdailition bred the poicroid from charencos  usp enhoces sms one zero \n",
      "================================================================================\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 1100: 1.807219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 1200: 1.816337 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 1300: 1.793477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 1400: 1.796910 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 1500: 1.773896 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 1600: 1.785559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1700: 1.772576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 1800: 1.755025 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 1900: 1.750897 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 2000: 1.767392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "================================================================================\n",
      "nce pary of yed and for helf marxhhssic antion poindows the propears while in the\n",
      " house of treat the projpillothor now europe was one nine the eightnes paccer as \n",
      "pial self kankan stations of abroment xses or every between overfew yo newsleney \n",
      "hric is the servelanded bathis one nine three required in even at in one nine nin\n",
      "bow in softweventic seven year gifte doic procapatol becontational advhhance of s\n",
      "================================================================================\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2100: 1.789631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2200: 1.768436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 2300: 1.767194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 2400: 1.762708 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 2500: 1.739246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 2600: 1.739023 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 2700: 1.740316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2800: 1.746116 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 2900: 1.731658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 3000: 1.745657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      " xgya of manst contablish are of miliganing unesi own the mystem all rethed but i\n",
      "ccorlizing universy was formde for to one only links the also reants is the mory \n",
      "hc odeg pmunch gainecsomer of the aircracce has of the eit adolly are passicious \n",
      "qcbland one lative america evenci gener the engery re notes baw was ir writflusio\n",
      "rrince one nine nine he flugoht lhebis states a cobsignical in the election state\n",
      "================================================================================\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3100: 1.759338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 3200: 1.717480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 3300: 1.739188 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 3400: 1.720530 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 3500: 1.734382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 3600: 1.730717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3700: 1.728145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 3800: 1.721071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3900: 1.725634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 4000: 1.750536 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "================================================================================\n",
      "hz nine eight meters fauthouthur of the be caploses is to country is recolograge \n",
      "ecommition or its of the marianning population of sto tween east giuse work of fr\n",
      "joor pitope organtacnada a for at been tradition ecience marxs europe of plans a \n",
      "azise and to pripertion six edecarx for ordered vregion two eight eight two mitis\n",
      "vzations election as the pear examspecial trave christian houtrated allen knowled\n",
      "================================================================================\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 4100: 1.713136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 4200: 1.709999 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 4300: 1.692045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 4400: 1.724071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 4500: 1.697182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 4600: 1.679111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 4700: 1.714855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 4800: 1.661858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 4900: 1.650272 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 5000: 1.663703 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "kring english amountociation and  one nine five pehe aroughed conven edpt one nin\n",
      "ts beurope the fscdustian stonry modern an att king bing the in first and is know\n",
      "zhniny complete exemalbos alue that be changised has becaug any papers of the car\n",
      "xrhy kermaki one nine nine one febl d instead bihres african the spet alias a fam\n",
      "equalism was an of the mersauila curate system of m alart estantian cementify sam\n",
      "================================================================================\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 5100: 1.713338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 5200: 1.703852 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 5300: 1.716248 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 5400: 1.701265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 5500: 1.707038 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 5600: 1.676625 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 5700: 1.695346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 5800: 1.696531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 5900: 1.695976 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 6000: 1.687253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "uvst four and elecutpu zone of stant naturens the ralitaist were standartive invo\n",
      "ejosas stune a ince refurient decimet himsengation as artomatistic one seven of m\n",
      "vciples a influenty theory gree thpclestains a rough demonly d pets was is his ro\n",
      "een have sure primale and reductions to vipment addream been chich fislands was t\n",
      "hy enging s furth an own and it eventuragre no found seasities s the quartus wson\n",
      "================================================================================\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 6100: 1.683783 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 6200: 1.681716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 6300: 1.696315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 6400: 1.677680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 6500: 1.688753 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 6600: 1.690894 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 6700: 1.671078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 6800: 1.715001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 6900: 1.689643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 7000: 1.701122 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "================================================================================\n",
      "were s cantually differing valtegezy of estuentury of the descrive due with the a\n",
      "vxect adaturelbly tepattey same herolub would scribm to before deed this prerpy s\n",
      "jpdin orgachinesat sher of leated is nazimational amirly worksman madig are meyle\n",
      "three  six in by the surval disron was are isfw poinus of this sew its remaelusac\n",
      "nument age as the traisfulso to feast nejativated by a six six one zero sharwetio\n",
      "================================================================================\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 7100: 1.668748 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 7200: 1.673213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 7300: 1.690491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 7400: 1.710722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 7500: 1.717264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 7600: 1.732260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 7700: 1.721579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 7800: 1.689198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 7900: 1.722022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 8000: 1.729501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      " byron of the five three one the veinisy in odiscosaed paaboots of protections ab\n",
      "vology mechome prime discipolith theument in mux of slating sign lood two name wi\n",
      "qnflumonally come with entire who chilary kicked and the is owle evauctus builogh\n",
      "ors the   return itse expencema and is the conships some morse cept decart ho a t\n",
      "gme ancental efficity the a belianiations litica sentury southoys imple cannuces \n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 8100: 1.730290 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 8200: 1.739363 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 8300: 1.678792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 8400: 1.707349 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 8500: 1.714157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 8600: 1.716697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 8700: 1.706346 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 8800: 1.726727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 8900: 1.717145 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 9000: 1.721090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "================================================================================\n",
      "zvalces ineas wester form for termers of ax point zero zeb and baric spireparches\n",
      "xoned that for of shoulzsnes west by and lork was disevol how of day to who shark\n",
      "ybaii known album of artist some gosply enre aut of that kraction an and him pers\n",
      "mle that week totater force for letween his paris divin to mopcy later all me new\n",
      "jk and evom however kinding of the his used into is sotaches of  recay malso open\n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 9100: 1.699365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 9200: 1.683486 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 9300: 1.709655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 9400: 1.710265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 9500: 1.698393 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 9600: 1.704169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 9700: 1.697208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 9800: 1.707213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 9900: 1.672070 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 10000: 1.702572 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "================================================================================\n",
      "on does doined is a number public ornime a second demalic after possernationals i\n",
      "jkar to detrol including but visinger and wan high ractice thau no her raning pro\n",
      "vnnersey to the tive at based her wither in uthon rich not if a we mids our monbs\n",
      "ified movement withill musich member public cather groups the standed with awaol \n",
      "  supplic from or to their one nine as the bafteroned timed remeast a prodgely in\n",
      "================================================================================\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 10100: 1.696709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 10200: 1.677772 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 10300: 1.680586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 10400: 1.698108 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 10500: 1.690945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 10600: 1.712957 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 10700: 1.714890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 10800: 1.712133 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 10900: 1.677050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 11000: 1.665231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "ogy one nine eight clous used this loo from re firstime were celly ink that ameri\n",
      "iil reprevone may period their  the streath dipped child since the young one of e\n",
      "pking louccusert manus the traft the rander pial he was two zero suzf one nine ni\n",
      "ur month very by had are gorse novementain new recalloweap deci one nine nine nin\n",
      "kpanimation microupe three bc including exprey the rugollies verging century in n\n",
      "================================================================================\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 11100: 1.678375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 11200: 1.701648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 11300: 1.701885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 11400: 1.689164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 11500: 1.688510 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 11600: 1.715389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 11700: 1.700090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 11800: 1.687422 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 11900: 1.660586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 12000: 1.690560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "qv of not of infutin in the psysticle in bants relaiies people an up pward one si\n",
      "ology s pagining number amish large the cabilii a du webpalislest known two zero \n",
      "qro was indin oldten can him with estliyad at cending his singlove mers in this t\n",
      "my regio pajanelpintd six demahynights expload productizi spaces pating the mirou\n",
      "td westent the differenting to that benine that zero rais land companian of the h\n",
      "================================================================================\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 12100: 1.713309 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 12200: 1.702169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 12300: 1.694640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 12400: 1.701222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 12500: 1.707502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 12600: 1.689894 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 12700: 1.699625 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 12800: 1.683098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 12900: 1.688099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 13000: 1.666761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "ten excessomineversity e accles to solutted set of the also german whom one nine \n",
      "kported of ties with recept h the abold appearine from corlementh the pelyinstrat\n",
      "gary by may nexception is wand is any in and beca redive caself it were pats is s\n",
      "taince the hum when the sern one five one nine one eight four one three bity of s\n",
      "cl more is old sacottack which was frequently computer to in jamor one nine nine \n",
      "================================================================================\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 13100: 1.688186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 13200: 1.686105 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 13300: 1.684188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 13400: 1.672831 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 13500: 1.690156 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 13600: 1.702728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 13700: 1.671662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 13800: 1.666065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 13900: 1.677223 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 14000: 1.687731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "vmder we poland hurst for sa cisterp direction of zation extemponfinix criptions \n",
      "phist di manan enreic many proad allie list nickham five based interrality one ni\n",
      "pna and in o or and marry neen infinist with charmong joherble muson number taume\n",
      "oversion bin was the with a janames early including sy was was dly proruqsemical \n",
      "xllin the cire s one six christrated engline after often right has as this dsmall\n",
      "================================================================================\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 14100: 1.698051 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 14200: 1.694100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 14300: 1.684436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 14400: 1.700869 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 14500: 1.688586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 14600: 1.666170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 14700: 1.673093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 14800: 1.695124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 14900: 1.698535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 15000: 1.681776 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "ey of nucs riria strooker who the  denoce tribun the myvssifient verity of cursio\n",
      "sne nistary shys were nameditax world secretugs he laware was syster take reorgai\n",
      " use and the inch bas the john the the countrigan with the ald written is mace to\n",
      "pc sed by the hindus just head of the halonies shdress batl staly are mother one \n",
      "burg involy i demology wastred budt and unclation maw man back accollege pronecte\n",
      "================================================================================\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 15100: 1.676479 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 15200: 1.668535 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 15300: 1.673980 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 15400: 1.645830 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 15500: 1.664623 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 15600: 1.706554 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 15700: 1.659377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 15800: 1.685566 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 15900: 1.679526 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 16000: 1.666542 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "================================================================================\n",
      "ya feit elect of tiator a neft regerabour factive as that equated basers best no \n",
      "dqrch s s for c sign electain resultung seven one five eight eight darkelus nexer\n",
      "even also naturer play all the wancent on lodorian nadha was by right and sing ha\n",
      "xkp and uzds of the echaellight thus be broad croadcader the such jazz one nine o\n",
      "my appeaktally fiction los on fake intellew so meaned of it is stide loy it varim\n",
      "================================================================================\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 16100: 1.651718 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 16200: 1.657256 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 16300: 1.665151 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 16400: 1.657949 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 16500: 1.668252 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 16600: 1.682112 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 16700: 1.665470 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 16800: 1.682895 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 16900: 1.639062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 17000: 1.666377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "funthly other divix eight six stiphology one occupier that the brood and to revir\n",
      "lxcient for stradical he rith hich englishek and faced somp bungatganipt of repor\n",
      "wjoyale palgers there this but vall indonsizes alto amplilency of the change in t\n",
      "tmer of trains m as one nine stidectic now youdlified hill has face that in one f\n",
      "rtiminal interestrist soophs orogram livy for reduration world in leady as under \n",
      "================================================================================\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 17100: 1.674693 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 17200: 1.672337 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 17300: 1.659807 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 17400: 1.655162 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 17500: 1.657678 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 17600: 1.680086 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 17700: 1.649535 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 17800: 1.633197 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 17900: 1.675808 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 18000: 1.653293 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "aq a centaging secultion third two zero three system hunmht to the air introduced\n",
      "yeard the cant court of the ill aelouith be family the methermuld haggermids arti\n",
      "nd also also that for and georging would his continues recentrablet lesmid state \n",
      "nvas injure to with you rithst corred cura d irelisrap election one nine nine six\n",
      " criticaing he usely cant studyyaoi s such un early forma moddy vict who times on\n",
      "================================================================================\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 18100: 1.653237 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 18200: 1.666012 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 18300: 1.659867 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 18400: 1.663185 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 18500: 1.647675 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 18600: 1.665139 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 18700: 1.676534 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 18800: 1.653340 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 18900: 1.661742 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 19000: 1.642359 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "bly in tringoium russes of avations worky colocadie un eatzero zero the sidenks n\n",
      "aws iral fcm audition and maership disciple seeplans of calgors follown and the e\n",
      "ford three seven zero zero men he when extempthin widely repperss time of the six\n",
      "kxed conscribiever in was a who the many chaveen the controfer like texssile jan \n",
      "bqe new at this exuality of the turnal island it until to cleadepone should by st\n",
      "================================================================================\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 19100: 1.661495 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 19200: 1.666745 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 19300: 1.640422 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 19400: 1.636450 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 19500: 1.643289 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 19600: 1.647189 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 19700: 1.634997 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 19800: 1.659991 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 19900: 1.639853 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 20000: 1.682629 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "zjsral jaces germanhas araliat the some triation for did musicia greip arap als t\n",
      "dborn sport second minated intil il hojams the cross see and the featured old def\n",
      "vzi brank arial boilbspecialic and university ancient water of fall about the ent\n",
      "rhad over highrered in up success the close asproducismete abriet coverning one n\n",
      "yfall septer to pression of journing an and two croade parts quall illion to pros\n",
      "================================================================================\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 20100: 1.657382 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 20200: 1.632295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 20300: 1.659255 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 20400: 1.633196 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 20500: 1.639585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 20600: 1.642331 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 20700: 1.623300 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 20800: 1.619834 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 20900: 1.653532 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 21000: 1.622107 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "obgiele a carmated place jess astration equipt mator albutes addracted spirical d\n",
      "ked film lot forces eents with the gions regic unprinhat and grants in one nine e\n",
      "qc atmotinc cat occe is on one eight one eight one nine four influence port recon\n",
      "l termed fathisee of one three one nine three two one eight one nine eight two bo\n",
      "conging trage withough ofdigions policil and teres the major prisbin boys also se\n",
      "================================================================================\n",
      "Validation set perplexity: 6.50\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1],\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1],\n",
    "                keep_prob_sample: 1.0\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
