{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = './data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taking the code from previous exercise & adding optimization \n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "\n",
    "  # ADDING REGULARIZATION TERM\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "\n",
    "  loss = (tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \n",
    "    beta * tf.nn.l2_loss(weights)) # beta * 1/2 * ||w||^2\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 48.701633\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.6%\n",
      "Minibatch loss at step 500: 0.746653\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1000: 0.800058\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 0.566515\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2000: 0.648468\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2500: 0.782561\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.784396\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 1e-2} \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beta parameter has to be tuned: run the following code and plot accuracy to tune beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_vals = np.logspace(-4, -1, num = 10)\n",
    "validation_acc = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : beta_val} \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        validation_acc.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAFqCAYAAAB73XKSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c9b1c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogx(beta_vals, validation_acc)\n",
    "plt.show()\n",
    "\n",
    "# Seems like 1e-3 is the best accuracy we can do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now doing the same thing for neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data portion is the same as before \n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables: turn into weights -> relu -> weights \n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation: insert relu's \n",
    "  relu_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  logits = tf.matmul(relu_train, weights_2) + biases_2\n",
    "\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  loss = (tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \n",
    "    beta * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_relu = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(valid_relu, weights_2) + biases_2)\n",
    "  test_relu = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_relu, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 659.898926\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 24.1%\n",
      "Minibatch loss at step 500: 200.913437\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1000: 115.125145\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1500: 68.987320\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 41.260166\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2500: 25.191193\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3000: 15.445206\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_vals = np.logspace(-4, -1, num = 10)\n",
    "validation_acc = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : beta_val} \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        validation_acc.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAFqCAYAAAB73XKSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyDq4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b1117d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy\n",
    "plt.semilogx(beta_vals, validation_acc)\n",
    "plt.show()\n",
    "\n",
    "# Seems like 1e-3 is the best accuracy we can do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 652.010010\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 36.0%\n",
      "Minibatch loss at step 50: 299.214691\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 100: 284.569122\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 150: 270.687347\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 200: 257.482422\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 250: 244.921951\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 300: 232.974121\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 350: 221.608734\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 400: 210.798203\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 450: 200.515137\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 500: 190.733337\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Test accuracy: 76.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 501\n",
    "num_batches = 4\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step % batch_size) \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minibatch accuracy is at 100%, while the validation and test accuracy are significantly lower. This indicates overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data portion is the same as before \n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables: turn into weights -> relu -> weights \n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation: insert relu's \n",
    "  relu_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  dropout = tf.nn.dropout(relu_train, 0.5)\n",
    "  logits = tf.matmul(dropout, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_relu = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(valid_relu, weights_2) + biases_2)\n",
    "  test_relu = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_relu, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 445.774902\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 27.7%\n",
      "Minibatch loss at step 50: 1.034931\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 150: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 200: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 250: 1.235884\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 300: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 350: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 400: 0.169040\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 450: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.8%\n",
      "Test accuracy: 80.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 501\n",
    "num_batches = 4\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step % batch_size) \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there is still a case of overfitting as most of the minibatch accuracy is set at 100%, but not all. This neural net is bit more general and has a better test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "hidden_nodes2 = 512\n",
    "beta = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data portion is the same as before \n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  \n",
    "  # Variables\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes],\n",
    "                       stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, hidden_nodes2], stddev=np.sqrt(2.0 / hidden_nodes)))\n",
    "  biases_2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
    "\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / hidden_nodes2)))\n",
    "  biases_3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation: insert relu's \n",
    "  layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  layer2 = tf.nn.relu(tf.matmul(layer1, weights_2) + biases_2)\n",
    "  logits = tf.matmul(layer2, weights_3) + biases_3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "    beta * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.1, global_step, 100000, 0.96, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.3).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_layer1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  valid_layer2 = tf.nn.relu(tf.matmul(valid_layer1, weights_2) + biases_2)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(valid_layer2, weights_3) + biases_3)\n",
    "\n",
    "  test_layer1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  test_layer2 = tf.nn.relu(tf.matmul(test_layer1, weights_2) + biases_2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_layer2, weights_3) + biases_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.517687\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 35.6%\n",
      "Minibatch loss at step 500: 1.272588\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1000: 1.177266\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1500: 0.799434\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2000: 0.688862\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2500: 0.684691\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 3000: 0.654243\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 3500: 0.651992\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4000: 0.496784\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 4500: 0.498756\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 5000: 0.527769\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 5500: 0.525356\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 6000: 0.606769\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 6500: 0.376457\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 7000: 0.552546\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 7500: 0.539975\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 8000: 0.610785\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 8500: 0.418050\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 9000: 0.498144\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.8%\n",
      "Test accuracy: 94.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
